{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Open AI GYM - Breakout"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "777cf29f7a87d7db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3386d98a0787c9b8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import time\n",
    "import random\n",
    "\n",
    "RANDOM_SEED = 5\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3117195c52f6b1af",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "game = \"ALE/Breakout-v5\"\n",
    "rendermode = \"rgb_array\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d8524fe0854630a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analysing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48cd40de1220d0d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "showing how the game screen looks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a4168eea9495e0a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "env = gym.make(game, render_mode=rendermode)\n",
    "\n",
    "\n",
    "print(\"Action Space: {}\".format(env.action_space))\n",
    "print(\"State space: {}\".format(env.observation_space))\n",
    "env.reset()\n",
    "img = plt.imshow(env.render())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26a8a84f39d8686d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Showing the actions and the total"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a16b02488a72985"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "action_meanings = env.unwrapped.get_action_meanings()\n",
    "num_actions = len(action_meanings)\n",
    "print(f\"Action Meanings: {action_meanings}, Total Actions: {num_actions}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df7a2120eb3ec88d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running a random game"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b52fc7a2037ee5e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "# Setup environment\n",
    "directory = './video_random_breakout'\n",
    "env = RecordVideo(gym.make(game, render_mode=rendermode), video_folder=directory)\n",
    "\n",
    "# Initialize variables\n",
    "epochs, rewards = 0, 0\n",
    "state = env.reset()\n",
    "\n",
    "# Run the game loop\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    rewards += reward\n",
    "    epochs += 1\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# Output the results\n",
    "print(f\"Number of steps: {epochs}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bdc69dd1098c2e7",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "average amount of steps in 100 games"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e1e3cf584a03ead"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Steps = []\n",
    "env = gym.make(game)\n",
    "for episode in range(100):\n",
    "    n_steps_episode = 0\n",
    "    total_training_rewards = 0\n",
    "    observation = env.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        new_observation, reward, done, truncated, info = env.step(action)\n",
    "        total_training_rewards += reward\n",
    "        observation = new_observation\n",
    "\n",
    "        n_steps_episode += 1.\n",
    "\n",
    "    Steps.append(n_steps_episode)\n",
    "\n",
    "np.mean(Steps)        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0828e4c2c8b47f8",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we define a function to plot the results. We will ask for two plots. In the first plot, all scores are plotted for the consecutive epsisodes. In the second plot, the frequencies of the rewards is given for the last 50 episodes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75c594d9e076a824"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_res(values, title=''):\n",
    "    ''' Plot the reward curve and histogram of results over time.'''\n",
    "\n",
    "    # Define the figure and axes\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    # Plot reward curve (score per episode)\n",
    "    ax1 = axes[0]\n",
    "    episodes = np.arange(1, len(values) + 1)\n",
    "    ax1.plot(episodes, values, label='Score per Episode', color='blue', alpha=0.7)\n",
    "    ax1.set_xlabel('Episodes', fontsize=12)\n",
    "    ax1.set_ylabel('Score', fontsize=12)\n",
    "    ax1.set_title('Training Progress', fontsize=14)\n",
    "    ax1.axhline(np.mean(values), linestyle='--', color='red', label='Average Score')\n",
    "    ax1.axhline(220, linestyle='--', color='green', label='Goal Score')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot histogram of scores from last 100 episodes\n",
    "    ax2 = axes[1]\n",
    "    ax2.hist(values[-100:], bins=20, color='green', alpha=0.7)\n",
    "    ax2.set_xlabel('Score Range', fontsize=12)\n",
    "    ax2.set_ylabel('Frequency', fontsize=12)\n",
    "    ax2.set_title('Score Distribution (Last 100 Episodes)', fontsize=14)\n",
    "    ax2.axvline(x=220, color='red', linestyle='--', label='Goal Score')\n",
    "    ax2.legend()\n",
    "\n",
    "    # Adjust layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'values' is a list of scores obtained per episode during training\n",
    "# plot_res(values, title='Training Results')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7f346199ad4bc9f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_res(Steps,'Random actions')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e946d86b4000d49c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e50f40d4c8ae6de"
  },
  {
   "cell_type": "markdown",
   "source": [
    "grayscaling + downscaling dimensions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43feec2e52df3ca0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def preprocess_state(state):\n",
    "    if isinstance(state, tuple):\n",
    "        state = state[0]  # Unpack the state from the tuple if necessary\n",
    "    state_gray = np.dot(state[..., :3], [0.299, 0.587, 0.114])  # Convert to grayscale\n",
    "    state_resized = state_gray[::2, ::2]  # Downsample by a factor of 2\n",
    "    state_resized = np.uint8(state_resized)  # Convert to uint8\n",
    "    state_resized = np.expand_dims(state_resized, axis=0)  # Add batch dimension\n",
    "    state_resized = np.expand_dims(state_resized, axis=-1)  # Add channel dimension\n",
    "    return state_resized"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcbdf8afbe28dce6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "env = gym.make(game, render_mode=rendermode)\n",
    "print(\"Action Space: {}\".format(env.action_space))\n",
    "print(\"State space: {}\".format(env.observation_space))\n",
    "\n",
    "state = env.reset()  # Reset the environment and get the initial state\n",
    "processed_state = preprocess_state(state)  # Preprocess the initial state\n",
    "\n",
    "# Display the preprocessed state\n",
    "plt.figure()\n",
    "plt.imshow(processed_state[0, ..., 0], cmap='gray')  # Show the first (and only) batch element as grayscale\n",
    "plt.title('Preprocessed Game State')\n",
    "plt.axis('off')  # Turn off axis labels\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4aba7e251d8b6583",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the DQN model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e01a2196687aea36"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, Input\n",
    "\n",
    "def create_dqn_model(input_shape, num_actions):\n",
    "    model = models.Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        layers.Conv2D(32, (8, 8), strides=(4, 4), activation='relu'),\n",
    "        layers.Conv2D(64, (4, 4), strides=(2, 2), activation='relu'),\n",
    "        layers.Conv2D(64, (3, 3), strides=(1, 1), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(num_actions)\n",
    "    ])\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27e35591287b733f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the Agent"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3474f6b473a8fc0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, model, epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.995, gamma=0.99):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(self.model.output_shape[-1])  # Random action\n",
    "        else:\n",
    "            q_values = self.model.predict(state)\n",
    "            return np.argmax(q_values[0])  # Greedy action\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "793fe5bd5935a9e4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the training loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e0e0dd02aa88c81"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Constants for training\n",
    "DEFAULT_NUM_EPISODES = 1000\n",
    "DEFAULT_LOG_INTERVAL = 100\n",
    "\n",
    "def train_dqn(agent, env, num_episodes=DEFAULT_NUM_EPISODES, log_interval=DEFAULT_LOG_INTERVAL):\n",
    "    scores = []\n",
    "    avg_scores = []\n",
    "\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state = env.reset()\n",
    "        state = preprocess_state(state)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            step_result = env.step(action)\n",
    "\n",
    "            if isinstance(step_result, tuple) and len(step_result) >= 3:\n",
    "                next_state, reward, done = step_result[:3]\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected step result: {step_result}\")\n",
    "\n",
    "            next_state = preprocess_state(next_state)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Store experience in replay buffer and perform model training (not shown)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        scores.append(total_reward)\n",
    "        agent.update_epsilon()\n",
    "\n",
    "        # Print progress and plot scores every log_interval episodes\n",
    "        if episode % log_interval == 0:\n",
    "            avg_score = np.mean(scores[-log_interval:])\n",
    "            print(f\"Episode {episode}/{num_episodes}, Average Score: {avg_score}, Epsilon: {agent.epsilon}\")\n",
    "            avg_scores.append(avg_score)\n",
    "            plot_training_progress(scores, avg_scores)\n",
    "\n",
    "    return scores\n",
    "\n",
    "def plot_training_progress(scores, avg_scores):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    episodes = np.arange(1, len(scores) + 1)\n",
    "    plt.plot(episodes, scores, label='Score per Episode', color='blue', alpha=0.7)\n",
    "    plt.plot(np.arange(log_interval, len(scores) + 1, log_interval), avg_scores, label=f'Average Score (per {log_interval} episodes)', color='orange')\n",
    "    plt.xlabel('Episodes', fontsize=12)\n",
    "    plt.ylabel('Score', fontsize=12)\n",
    "    plt.title('Training Progress', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Usage example:\n",
    "# Assuming 'agent' and 'env' are defined and initialized as in your code snippet\n",
    "scores = train_dqn(agent, env)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92efcc6ca16b9cd2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define input shape and number of actions\n",
    "input_shape = (105, 80, 1)  # Example input shape (height, width, channels)\n",
    "num_actions = env.action_space.n  # Number of actions in the environment\n",
    "\n",
    "# Create the DQN model\n",
    "dqn_model = create_dqn_model(input_shape, num_actions)\n",
    "\n",
    "# Create the DQNAgent\n",
    "agent = DQNAgent(model=dqn_model)\n",
    "\n",
    "# Train the DQN agent\n",
    "scores = train_dqn(agent, env)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad749863b1859cac",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ed584e43f5173a02"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5394a49bb689ea64"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the path where you want to save the model\n",
    "model_save_path = \"DQN_Model.keras\"\n",
    "\n",
    "# Save the model to the specified path\n",
    "agent.model.save(model_save_path)\n",
    "\n",
    "print(\"Trained model saved to:\", model_save_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4947428bb750f521",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualise Training results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "757868a789afeaf8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_res(scores, title='DQN Training Results')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41d3a8359f60fc96",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import RecordVideo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Define your game and rendering mode\n",
    "game = \"ALE/Breakout-v5\"\n",
    "render_mode = \"rgb_array\"\n",
    "\n",
    "# Create the Breakout environment (without video recording)\n",
    "env = gym.make(game, render_mode=render_mode)\n",
    "\n",
    "# Load your trained DQN model\n",
    "dqn_model = load_model(model_save_path)\n",
    "\n",
    "# Create a video recording folder\n",
    "directory = './video_dqn_breakout'\n",
    "env = RecordVideo(env, video_folder=directory)\n",
    "\n",
    "# Initialize variables\n",
    "epochs, rewards = 0, 0\n",
    "state = env.reset()\n",
    "\n",
    "# Run the game loop with your trained DQN model\n",
    "while True:\n",
    "    # Preprocess the current state\n",
    "    state_preprocessed = preprocess_state(state)\n",
    "\n",
    "    # Use the DQN model to choose an action\n",
    "    action = np.argmax(dqn_model.predict(state_preprocessed))\n",
    "\n",
    "    # Take the chosen action\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    # Accumulate rewards\n",
    "    rewards += reward\n",
    "    epochs += 1\n",
    "\n",
    "    # Check if the episode is done\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# Output the results\n",
    "print(f\"Number of steps: {epochs}, Total rewards: {rewards}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47ed37c1bf552264",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "35a7ccadd6d3089f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
